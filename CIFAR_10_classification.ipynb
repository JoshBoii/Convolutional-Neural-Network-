{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshBoii/Convolutional-Neural-Network-/blob/main/CIFAR_10_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pFAcDZiT0X3d"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries and modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from itertools import product\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXveatKZZMNY",
        "outputId": "5d01f0ff-e397-4ed3-a447-fd9816026f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr  4 21:49:19 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   29C    P0    45W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr-vm7_ccCFa"
      },
      "source": [
        "batch_size = 128 #64, 256, or 512, to see if they improve model performance\n",
        "num_epochs = 10 #Increasing the number of epochs might give the model more time to learn. Try increasing it to 20, 50, or 100 later,\n",
        "comment out poor accuracy hyperparameters\n",
        "\n",
        "`num_blocks` = number of blocks in the network, each block consists of several convolutional layers that extract features from the input.\n",
        "\n",
        "Increasing the number of blocks can increase the model's capacity to learn complex features, but also increases the risk of **overfitting** the training data if the model becomes too complex.\n",
        "\n",
        "`num_convs` = the number of convolutional layers within each block. \n",
        "\n",
        "Increasing the number of convolutions can also increase the model's capacity to learn complex features, but also increases the risk of **overfitting**.\n",
        "\n",
        "`num_classes`= the number of output classes that the model can predict. \n",
        "\n",
        "Increasing the number of classes will make the problem more complex, and thus the model will require more capacity to learn the mapping between inputs and outputs. However, if the dataset is too small for the number of classes, the model might not be able to learn the necessary features to discriminate between the classes, which can lead to poor accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3B_loKC75qHn"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "num_classes = 10\n",
        "batch_size = 128 \n",
        "num_epochs = 10 "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "num_blocks = 3\n",
        "num_convs = 2\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "num_blocks_values = list(range(2, 7)) # 2 to 6 as (2, 7)\n",
        "num_convs_values = list(range(1, 5)) # 1 to 4 as (1, 5)\n",
        "learning_rate_values = [0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
        "\n",
        "Footnote\n",
        "\n",
        "[1] trial run showed 0.00005 and 0.01 give poor accuracy; very low learning \n",
        "rates like 0.00005 result in slow training + poor convergence, while very high learning rates like 0.01 may cause the model to overshoot the optimal weights and result in poor accuracy.\n",
        "\n",
        "[2]  too few blocks (2) do not provide enough depth for the model to learn complex features and too many blocks (6) may cause the model to \"overfit\" to training data."
      ],
      "metadata": {
        "id": "h0tGgWXK7s6u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UC80rUspG8dt"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters to tune\n",
        "num_blocks_values = list(range(4, 6)) # 2 to 6 as (2, 7)\n",
        "num_convs_values = list(range(1, 5)) # 1 to 4 as (1, 5)\n",
        "learning_rate_values = [0.0005, 0.001, 0.005]\n",
        "\n",
        "# Create a list of all hyperparameter combinations\n",
        "hyperparams = list(product(num_blocks_values, num_convs_values, learning_rate_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DAdDl3d7PPAk"
      },
      "outputs": [],
      "source": [
        "#Read the dataset and create dataloaders\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKvsmJxRPPqO",
        "outputId": "59da60ea-8712-4e00-8a31-6dae5d53eccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 106381870.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IHnK_zBlPbli"
      },
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x9KE_nfS_MMH"
      },
      "outputs": [],
      "source": [
        "# Define the model architecture \n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_channels, K, dropout_rate=0.5):\n",
        "        super(Block, self).__init__()\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(in_channels, K)\n",
        "        self.convs = nn.ModuleList(\n",
        "            [nn.Sequential(\n",
        "                nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(in_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ) for _ in range(K)\n",
        "            ])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.avgpool(x)\n",
        "        out = torch.flatten(out, 1)\n",
        "        a = self.fc(out)\n",
        "        a = torch.softmax(a, dim=1).unsqueeze(2).unsqueeze(3)\n",
        "        conv_outs = [a[:, i:i + 1] * conv(x) for i, conv in enumerate(self.convs)]\n",
        "        out = sum(conv_outs)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_blocks, K, num_classes, dropout_rate):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.blocks = nn.Sequential(*[Block(64, K) for _ in range(num_blocks)])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)  # Add dropout here\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oj7ALEoRKLC"
      },
      "source": [
        "The line `device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")` checks if a GPU with CUDA support is available on your system. If it is available, it will use the GPU (\"cuda\") as the device. If not, it will fall back to using the CPU (\"cpu\"). This allows the code to run on systems with or without GPU support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9-ddV10_JVU5"
      },
      "outputs": [],
      "source": [
        "# Define a function to train and evaluate the model with a given set of hyperparameters\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == targets.data)\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def test(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, targets)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == targets.data)\n",
        "    epoch_loss = running_loss / len(test_loader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(test_loader.dataset)\n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "THE OUTPUT \n",
        "list of tuples containing 5 values: the epoch number, training loss, training accuracy, testing loss, and testing accuracy"
      ],
      "metadata": {
        "id": "9Qo70qVaSCgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to train and evaluate the model with a given set of hyperparameters 2\n",
        "def train_and_evaluate(hyperparams):\n",
        "    num_blocks, K, learning_rate = hyperparams\n",
        "    model = Model(num_blocks, K, num_classes, dropout_rate).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "        test_loss, test_acc = test(model, test_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {train_loss:.4f} Acc: {train_acc:.4f} Test Loss: {test_loss:.4f} Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "        # Store loss and accuracy at each epoch\n",
        "        losses.append((epoch, train_loss, train_acc.item(), test_loss, test_acc.item()))\n",
        "\n",
        "    return losses, test_acc, hyperparams\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "results = []\n",
        "dropout_rate = 0.5  # Adjust this value as needed\n",
        "\n",
        "for idx, (num_blocks, num_convs, learning_rate) in enumerate(hyperparams):\n",
        "    model = Model(num_blocks, num_convs, num_classes, dropout_rate).to(device)\n",
        "    accuracy = train_and_evaluate((num_blocks, num_convs, learning_rate))\n",
        "    print(f\"Combination {idx + 1}/{len(hyperparams)}: {num_blocks} blocks, {num_convs} convs, {learning_rate} lr, Accuracy: {accuracy:.4f}\")\n",
        "    results.append((accuracy, (num_blocks, num_convs, learning_rate)))\n",
        "\n",
        "# Evaluate all hyperparameter combinations - grid search\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dropout_rate = 0.5  # Adjust this value as needed\n",
        "results = []\n",
        "\n",
        "for hyperparams in hyperparams:\n",
        "    losses, test_acc, best_hyperparams = train_and_evaluate(hyperparams)\n",
        "    results.append((losses, test_acc, best_hyperparams))\n",
        "\n",
        "# Find the best accuracy and the corresponding hyperparameters\n",
        "best_acc = max(results, key=lambda x: x[1])[1]\n",
        "best_params = max(results, key=lambda x: x[1])[2]\n",
        "\n",
        "print(\"Best accuracy:\", best_acc)\n",
        "print(\"Best parameters:\", best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "R119ENRkPiKe",
        "outputId": "6f3420ef-4177-4234-afdf-c65b4ab34b16"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 2.0286 Acc: 0.2486 Test Loss: 1.7775 Test Acc: 0.3300\n",
            "Epoch 2/10 - Loss: 1.7337 Acc: 0.3401 Test Loss: 1.5384 Test Acc: 0.4257\n",
            "Epoch 3/10 - Loss: 1.6397 Acc: 0.3834 Test Loss: 1.4373 Test Acc: 0.4536\n",
            "Epoch 4/10 - Loss: 1.5846 Acc: 0.4059 Test Loss: 1.3986 Test Acc: 0.4755\n",
            "Epoch 5/10 - Loss: 1.5422 Acc: 0.4222 Test Loss: 1.3630 Test Acc: 0.4901\n",
            "Epoch 6/10 - Loss: 1.5066 Acc: 0.4388 Test Loss: 1.3369 Test Acc: 0.5042\n",
            "Epoch 7/10 - Loss: 1.4750 Acc: 0.4539 Test Loss: 1.2664 Test Acc: 0.5372\n",
            "Epoch 8/10 - Loss: 1.4485 Acc: 0.4676 Test Loss: 1.3551 Test Acc: 0.5127\n",
            "Epoch 9/10 - Loss: 1.4167 Acc: 0.4798 Test Loss: 1.2831 Test Acc: 0.5309\n",
            "Epoch 10/10 - Loss: 1.4028 Acc: 0.4857 Test Loss: 1.2160 Test Acc: 0.5483\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-605d9f0dbfd0>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_convs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_convs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Combination {idx + 1}/{len(hyperparams)}: {num_blocks} blocks, {num_convs} convs, {learning_rate} lr, Accuracy: {accuracy:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_convs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to tuple.__format__"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('results.pkl', 'wb') as f:\n",
        "    pickle.dump(results, f)\n"
      ],
      "metadata": {
        "id": "uiIJfqQvRR_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the training loop, save the results to a file named results.pkl:\n",
        "python\n",
        "Copy code\n",
        "\n",
        "`with open('results.pkl', 'wb') as f:`\n",
        "    `pickle.dump(results, f)`\n",
        "\n",
        "To load the results from the file later, you can use the following code:\n",
        "python\n",
        "Copy code\n",
        "\n",
        "`with open('results.pkl', 'rb') as f:`\n",
        "    `loaded_results = pickle.load(f)`\n",
        "    \n",
        "Now, loaded_results will contain the results you saved previously, and you can use them for further analysis without having to rerun the training process.\n",
        "\n"
      ],
      "metadata": {
        "id": "jxGlpfU8RTd4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bsUv4oCgLq-"
      },
      "outputs": [],
      "source": [
        "# Evaluate all hyperparameter combinations - grid search\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dropout_rate = 0.5  # Adjust this value as needed\n",
        "results = []\n",
        "\n",
        "for hyperparams in hyperparams:\n",
        "    losses, test_acc, best_hyperparams = train_and_evaluate(hyperparams)\n",
        "    results.append((losses, test_acc, best_hyperparams))\n",
        "\n",
        "# Find the best accuracy and the corresponding hyperparameters\n",
        "best_acc = max(results, key=lambda x: x[1])[1]\n",
        "best_params = max(results, key=lambda x: x[1])[2]\n",
        "\n",
        "print(\"Best accuracy:\", best_acc)\n",
        "print(\"Best parameters:\", best_params)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBv7_NvvUK5I"
      },
      "source": [
        "calculated epoch accuracies were stored in a separate text file which prevents having to re run a long calculation of 180 combinations of hyperparameters (5 num_blocks * 4 num_convs * 6 learning_rates) while training the model for 10 epochs for each combination. This saves time with GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "graph to compare hyperparameters and accuracy "
      ],
      "metadata": {
        "id": "v2FTDYBO41jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracies for each hyperparameter combination\n",
        "accuracies = [r[1] for r in results]\n",
        "hyperparams_str = [f\"{r[2][0]} blocks, {r[2][1]} convs, {r[2][2]} lr\" for r in results]\n",
        "\n",
        "# Create a dictionary to store hyperparameters and their corresponding accuracies\n",
        "accuracy_dict = {hp_str: acc for hp_str, acc in zip(hyperparams_str, accuracies)}\n",
        "\n",
        "# Create a line graph\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(hyperparams_str, accuracies, marker='o', linestyle='-', markersize=6)\n",
        "\n",
        "plt.xlabel(\"Hyperparameter Combination\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy for Different Hyperparameter Combinations\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "9Dw33tTsKMgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rotation parameter is an optional argument in the plt.xticks() function that helps rotate the x-axis labels. By default, the rotation value is set to None, which means the labels will be displayed horizontally. However, if you have many hyperparameter combinations or long labels, the x-axis labels can become cluttered and overlap with each other, making them difficult to read.\n",
        "\n",
        "To improve readability, you can use the rotation parameter to set the angle (in degrees) for rotating the labels. For instance, you can set rotation=45 to rotate the labels 45 degrees counterclockwise:\n",
        "\n",
        "Copy code\n",
        "`plt.xticks(rotation=45)`\n",
        "Adjust the rotation value to achieve the best appearance and readability for your specific graph."
      ],
      "metadata": {
        "id": "swoB2jpnO9sW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqjjlGM-DIy_"
      },
      "source": [
        "**END**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}